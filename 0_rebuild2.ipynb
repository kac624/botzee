{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnvironment:\n",
    "    def __init__(self):\n",
    "        self.hand = []\n",
    "        self.score_potentials = []\n",
    "        self.rolls_left = 3\n",
    "        self.score_card = {\n",
    "            'ones': -1, 'twos': -1, 'threes': -1,\n",
    "            'fours': -1, 'fives': -1, 'sixes': -1,\n",
    "            'three_of_a_kind': -1, 'four_of_a_kind': -1, 'full_house': -1,\n",
    "            'small_straight': -1, 'large_straight': -1,\n",
    "            'yahtzee': -1, 'chance': -1,\n",
    "            'total_score': 0\n",
    "        }\n",
    "        self.label_score_dict = {\n",
    "            0: 'ones', 1: 'twos', 2: 'threes', 3: 'fours', 4: 'fives', 5: 'sixes',\n",
    "            6: 'three_of_a_kind', 7: 'four_of_a_kind', 8: 'full_house',\n",
    "            9: 'small_straight', 10: 'large_straight', 11: 'yahtzee', 12: 'chance'\n",
    "        }\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.hand + [self.rolls_left] + list(self.score_card.values())\n",
    "    \n",
    "    def get_score_card(self):\n",
    "        return self.score_card.copy()\n",
    "\n",
    "    def roll_dice(self):\n",
    "        num_dice_to_roll = 5 - len(self.hand)\n",
    "        rolled_dice = [random.randint(1, 6) for _ in range(num_dice_to_roll)]\n",
    "        self.hand = self.hand + rolled_dice\n",
    "        self.rolls_left -= 1\n",
    "        self.get_potential_scores()\n",
    "\n",
    "    def update_hand(self, action_scalar, length=5):\n",
    "        binary_string = f\"{action_scalar:0{length}b}\"\n",
    "        decoded_action = torch.tensor([float(bit) for bit in binary_string])\n",
    "        self.hand = [self.hand[x] for x in range(5) if decoded_action[x] == 1]\n",
    "    \n",
    "    def get_potential_scores(self):\n",
    "        # Scoring logic\n",
    "        potential_scores = {\n",
    "            'ones': sum([x for x in self.hand if x == 1]),\n",
    "            'twos': sum([x for x in self.hand if x == 2]),\n",
    "            'threes': sum([x for x in self.hand if x == 3]),\n",
    "            'fours': sum([x for x in self.hand if x == 4]),\n",
    "            'fives': sum([x for x in self.hand if x == 5]),\n",
    "            'sixes': sum([x for x in self.hand if x == 6]),\n",
    "            'three_of_a_kind': sum(self.hand) if (\n",
    "                np.unique(self.hand, return_counts = True)[1].max() >= 3\n",
    "            ) else 0,\n",
    "            'four_of_a_kind': sum(self.hand) if (\n",
    "                np.unique(self.hand, return_counts = True)[1].max() >= 4\n",
    "            ) else 0,\n",
    "            'full_house': 25 if (\n",
    "                (np.unique(self.hand, return_counts = True)[1].max() == 3) &\n",
    "                (np.unique(self.hand, return_counts = True)[1].min() == 2)\n",
    "            ) else 0,\n",
    "            'small_straight': 30 if (\n",
    "                all(number in self.hand for number in [1,2,3,4]) or\n",
    "                all(number in self.hand for number in [2,3,4,5]) or\n",
    "                all(number in self.hand for number in [3,4,5,6])\n",
    "            ) else 0,\n",
    "            'large_straight': 40 if (\n",
    "                all(number in self.hand for number in [1,2,3,4,5]) or\n",
    "                all(number in self.hand for number in [2,3,4,5,6])\n",
    "            ) else 0,\n",
    "            'yahtzee': 50 if len(set(self.hand)) == 1 else 0,\n",
    "            'chance': sum(self.hand)\n",
    "        }\n",
    "        # If a score is already marked, remove it from potential scores\n",
    "        for score_type in potential_scores.keys():\n",
    "            if self.score_card[score_type] != -1:\n",
    "                potential_scores[score_type] = 0\n",
    "        self.score_potentials.append(potential_scores)\n",
    "\n",
    "    def calc_reward_dice(self, pre_potentials, post_potentials):\n",
    "        reward = sum(post_potentials.values()) - sum(pre_potentials.values())\n",
    "        # offset = 1\n",
    "        # base = 10\n",
    "        # reward = np.sign(reward) * np.log(abs(reward) + offset) / np.log(base)\n",
    "        return reward\n",
    "    \n",
    "    def calc_reward_score(self, pre_score_card, post_score_card, potential_scores, score_decision, score_amount):\n",
    "        reward = score_amount\n",
    "        # offset = 1\n",
    "        # base = 10\n",
    "        # reward = np.sign(reward) * np.log(abs(reward) + offset) / np.log(base)\n",
    "        return reward\n",
    "    \n",
    "    def mark_score(self, chosen_score_type, chosen_score):\n",
    "        self.score_card[chosen_score_type] = chosen_score\n",
    "        self.score_card['total_score'] += chosen_score\n",
    "\n",
    "    def reset_turn(self):\n",
    "        self.hand = []\n",
    "        self.score_potentials = []\n",
    "        self.rolls_left = 3\n",
    "\n",
    "    def reset_game(self):\n",
    "        self.__init__()\n",
    "        return self.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(DiceModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        \n",
    "        x = self.output_layer(x) # torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "class ScoreModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size, device):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, int(action), reward, next_state, done))\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self, state_size_dice, action_size_dice, state_size_score, action_size_score, hidden_sizes, \n",
    "        seed, device, buffer_size=10000, batch_size=64, gamma=0.99, lr=0.001, tau=1e-3, learn_every=4, learn_num=3\n",
    "    ):\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize networks for keeping dice\n",
    "        self.qnetwork_local_dice = DiceModel(state_size_dice, hidden_sizes, action_size_dice).to(self.device)\n",
    "        self.qnetwork_target_dice = DiceModel(state_size_dice, hidden_sizes, action_size_dice).to(self.device)\n",
    "        self.optimizer_dice = optim.Adam(self.qnetwork_local_dice.parameters(), lr=lr)\n",
    "\n",
    "        # Initialize networks for keeping score\n",
    "        self.qnetwork_local_score = ScoreModel(state_size_score, hidden_sizes, action_size_score).to(self.device)\n",
    "        self.qnetwork_target_score = ScoreModel(state_size_score, hidden_sizes, action_size_score).to(self.device)\n",
    "        self.optimizer_score = optim.Adam(self.qnetwork_local_score.parameters(), lr=lr)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory_dice = ReplayBuffer(buffer_size, batch_size, self.device)\n",
    "        self.memory_score = ReplayBuffer(buffer_size, batch_size, self.device)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learn_every = learn_every\n",
    "        self.learn_num = learn_num\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done, network_type):\n",
    "        # Save experience in replay memory\n",
    "        if network_type == 'dice':\n",
    "            memory = self.memory_dice\n",
    "        elif network_type == 'score':\n",
    "            memory = self.memory_score\n",
    "        memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps\n",
    "        self.t_step = (self.t_step + 1) % self.learn_every\n",
    "        if self.t_step == 0:\n",
    "            if len(memory) > self.batch_size:\n",
    "                for _ in range(self.learn_num):\n",
    "                    experiences = memory.sample()\n",
    "                    self.learn(experiences, self.gamma, network_type)\n",
    "\n",
    "    def act_dice(self, state, eps=0.): # , thresold=0.5):\n",
    "        # Exploration\n",
    "        if random.random() < eps:\n",
    "            action = torch.randint(0, 32, (1,)).item()\n",
    "        # Exploitation\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            self.qnetwork_local_dice.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = self.qnetwork_local_dice(state)\n",
    "                probs = torch.softmax(logits, dim=0)\n",
    "                action = torch.argmax(probs).item()\n",
    "            self.qnetwork_local_dice.train()\n",
    "        return action\n",
    "\n",
    "    def act_score(self, state, eps=0.):\n",
    "        # Determine which score types are already taken\n",
    "        invalid_actions = [x for x in range(13) if state[6:-1][x] != -1]\n",
    "        # Exploration - random choice of valid actions\n",
    "        if random.random() < eps:\n",
    "            valid_actions = [x for x in range(13) if x not in invalid_actions]\n",
    "            action = random.choice(valid_actions)\n",
    "        # Exploitation - choose action with highest probability\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            self.qnetwork_local_score.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = self.qnetwork_local_score(state)\n",
    "                probs = torch.softmax(logits, dim=0)\n",
    "                # Remove invalid actions\n",
    "                mask = torch.ones(13).to(self.device)\n",
    "                mask[invalid_actions] = 0\n",
    "                probs = probs * mask\n",
    "                # Re-normalize\n",
    "                probs = probs / probs.sum()  \n",
    "                action = torch.argmax(probs).item()\n",
    "            self.qnetwork_local_score.train()\n",
    "        return action\n",
    "\n",
    "    def learn(self, experiences, gamma, network_type):\n",
    "        if network_type == 'dice':\n",
    "            local_network = self.qnetwork_local_dice\n",
    "            target_network = self.qnetwork_target_dice\n",
    "            optimizer = self.optimizer_dice\n",
    "            loss_func = nn.MSELoss() # nn.CrossEntropyLoss() # nn.BCELoss()\n",
    "        elif network_type == 'score':\n",
    "            local_network = self.qnetwork_local_score\n",
    "            target_network = self.qnetwork_target_score\n",
    "            optimizer = self.optimizer_score\n",
    "            loss_func = nn.MSELoss() # nn.CrossEntropyLoss()\n",
    "        # Unpack experiences into separate lists\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = target_network(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards.unsqueeze(1) + (gamma * Q_targets_next * (1 - dones.unsqueeze(1))) # rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = local_network(states).gather(1, actions.unsqueeze(1))\n",
    "        # Compute loss\n",
    "        loss = loss_func(Q_expected, Q_targets)\n",
    "        # Backpropogate and update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Soft-update target model\n",
    "        self.soft_update(local_network, target_network, self.tau)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data) # or + tau?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 256 # 64\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0005 # 0.001\n",
    "TAU = 1e-3 # 1e-4\n",
    "LEARN_EVERY = 4 \n",
    "LEARN_NUM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5000\tAverage Score: 49.39\tEpsilon: 0.96\tTime - 12.97 min\n",
      "Episode 10000\tAverage Score: 46.07\tEpsilon: 0.91\tTime - 26.03 min\n",
      "Episode 15000\tAverage Score: 49.37\tEpsilon: 0.86\tTime - 39.33 min\n",
      "Episode 20000\tAverage Score: 46.44\tEpsilon: 0.82\tTime - 51.13 min\n",
      "Episode 25000\tAverage Score: 45.75\tEpsilon: 0.78\tTime - 61.65 min\n",
      "Episode 30000\tAverage Score: 45.76\tEpsilon: 0.74\tTime - 71.79 min\n",
      "Episode 35000\tAverage Score: 45.08\tEpsilon: 0.71\tTime - 83.32 min\n",
      "Episode 40000\tAverage Score: 44.87\tEpsilon: 0.67\tTime - 95.17 min\n",
      "Episode 45000\tAverage Score: 46.14\tEpsilon: 0.64\tTime - 106.45 min\n",
      "Episode 50000\tAverage Score: 46.73\tEpsilon: 0.61\tTime - 118.66 min\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment\n",
    "env = GameEnvironment()\n",
    "\n",
    "# Set variables for agent\n",
    "state_size_dice = 20\n",
    "action_size_dice = 32\n",
    "state_size_score = 20\n",
    "action_size_score = 13\n",
    "HIDDEN_SIZES = [256, 256, 256, 128, 128, 128, 64, 64]\n",
    "\n",
    "# Initialize cuda and agent\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "agent = DQNAgent(\n",
    "    state_size_dice, action_size_dice, state_size_score, action_size_score, HIDDEN_SIZES, seed=0, device=device,\n",
    "    buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA, lr=LEARNING_RATE, tau=TAU, learn_every=LEARN_EVERY, learn_num=LEARN_NUM\n",
    ")\n",
    "init_local_params_dice = copy.deepcopy(agent.qnetwork_local_dice.state_dict())\n",
    "init_local_params_score = copy.deepcopy(agent.qnetwork_local_score.state_dict())\n",
    "init_target_params_dice = copy.deepcopy(agent.qnetwork_target_dice.state_dict())\n",
    "init_target_params_score = copy.deepcopy(agent.qnetwork_target_score.state_dict())\n",
    "\n",
    "'''Training loop variables'''\n",
    "n_episodes = 10 # 10**4 * 5\n",
    "eps_start = 1.0\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.995 # 0.995\n",
    "\n",
    "'''Start of training loop function'''\n",
    "# Initialize variables\n",
    "scores = []\n",
    "scores_window = deque(maxlen=100)\n",
    "eps = eps_start\n",
    "start = time.time()\n",
    "\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    state = env.reset_game()\n",
    "    score = 0\n",
    "    done = False\n",
    "\n",
    "    for turn in range(13):\n",
    "        env.reset_turn()\n",
    "        \n",
    "        # First roll\n",
    "        env.roll_dice()\n",
    "\n",
    "        # First dice action - choose dice\n",
    "        state = env.get_state()\n",
    "        action_dice = agent.act_dice(state, eps)\n",
    "        env.update_hand(action_dice)\n",
    "\n",
    "        # Second roll\n",
    "        env.roll_dice()\n",
    "\n",
    "        # Calculate reward and step\n",
    "        next_state = env.get_state()\n",
    "        reward_dice = env.calc_reward_dice(env.score_potentials[0], env.score_potentials[1])\n",
    "        agent.step(state, action_dice, reward_dice, next_state, done, 'dice')\n",
    "        state = next_state\n",
    "\n",
    "        # Second dice action - choose dice\n",
    "        action_dice = agent.act_dice(state, eps)\n",
    "        env.update_hand(action_dice)\n",
    "\n",
    "        # Final roll\n",
    "        env.roll_dice()\n",
    "        done = True if turn == 12 else False\n",
    "\n",
    "        # Calculate reward and step\n",
    "        next_state = env.get_state()\n",
    "        reward_dice = env.calc_reward_dice(env.score_potentials[1], env.score_potentials[2])\n",
    "        agent.step(state, action_dice, reward_dice, next_state, done, 'dice')\n",
    "        state = next_state\n",
    "\n",
    "        # Score action - choose score\n",
    "        pre_score_card = env.get_score_card()\n",
    "        action_score = agent.act_score(state, eps)\n",
    "\n",
    "        # Mark score\n",
    "        score_decision = env.label_score_dict[action_score]\n",
    "        score_amount = env.score_potentials[2][score_decision]\n",
    "        env.mark_score(score_decision, score_amount)\n",
    "        post_score_card = env.get_score_card()\n",
    "\n",
    "        # Calculate reward and step\n",
    "        next_state = env.get_state()\n",
    "        reward_score = env.calc_reward_score(pre_score_card, post_score_card, env.score_potentials[2], score_decision, score_amount)\n",
    "        agent.step(state, action_score, reward_score, next_state, done, 'score')\n",
    "\n",
    "    score = env.score_card['total_score']\n",
    "    scores_window.append(score)\n",
    "    scores.append(score)\n",
    "\n",
    "    if i_episode % ((n_episodes/100) + 1) == 0:\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "\n",
    "    print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\\tEpsilon: {eps:.2f}\\tTime - {(time.time() - start) / 60:.2f} min', end=\"\")\n",
    "    \n",
    "    if i_episode % (n_episodes/10) == 0:\n",
    "        print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\\tEpsilon: {eps:.2f}\\tTime - {(time.time() - start) / 60:.2f} min')\n",
    "    \n",
    "    if np.mean(scores_window) >= 100.0:\n",
    "        print(f'\\nEnvironment solved in {i_episode-100} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "        torch.save(agent.qnetwork_local_dice.state_dict(), 'checkpoint_keep.pth')\n",
    "        torch.save(agent.qnetwork_local_score.state_dict(), 'checkpoint_score.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_potential_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('ones', 2), ('twos', 2), ('threes', 3), ('fours', 4), ('fives', 0), ('sixes', 0), ('three_of_a_kind', 0), ('four_of_a_kind', 0), ('full_house', 0), ('small_straight', 0), ('large_straight', 0), ('yahtzee', 0), ('chance', 22), ('total_score', 33)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.score_card.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences, gamma, network_type = agent.memory_score.sample(), GAMMA, 'score'    \n",
    "\n",
    "if network_type == 'dice':\n",
    "    local_network = agent.qnetwork_local_dice\n",
    "    target_network = agent.qnetwork_target_dice\n",
    "    optimizer = agent.optimizer_dice\n",
    "    loss_func = nn.MSELoss() # nn.CrossEntropyLoss()\n",
    "elif network_type == 'score':\n",
    "    local_network = agent.qnetwork_local_score\n",
    "    target_network = agent.qnetwork_target_score\n",
    "    optimizer = agent.optimizer_score\n",
    "    loss_func = nn.MSELoss() # nn.CrossEntropyLoss()\n",
    "# Unpack experiences into separate lists\n",
    "states, actions, rewards, next_states, dones = experiences\n",
    "# Get max predicted Q values (for next states) from target model\n",
    "Q_targets_next = target_network(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "Q_targets = rewards.unsqueeze(1) + (gamma * Q_targets_next * (1 - dones.unsqueeze(1))) # rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "# Get expected Q values from local model\n",
    "Q_expected = local_network(states).gather(1, actions.unsqueeze(1))\n",
    "# Compute loss\n",
    "loss = loss_func(Q_expected, Q_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_local_params_dice = copy.deepcopy(agent.qnetwork_local_dice.state_dict())\n",
    "current_local_params_score = copy.deepcopy(agent.qnetwork_local_score.state_dict())\n",
    "current_target_params_dice = copy.deepcopy(agent.qnetwork_target_dice.state_dict())\n",
    "current_target_params_score = copy.deepcopy(agent.qnetwork_target_score.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "local -- dice --\n",
      "Parameter: input_layer.weight, Mean Absolute Difference: 0.12982723116874695\n",
      "Parameter: input_layer.bias, Mean Absolute Difference: 0.13248443603515625\n",
      "Parameter: hidden_layers.0.weight, Mean Absolute Difference: 0.115223728120327\n",
      "Parameter: hidden_layers.0.bias, Mean Absolute Difference: 0.10505426675081253\n",
      "Parameter: hidden_layers.1.weight, Mean Absolute Difference: 0.11448322236537933\n",
      "Parameter: hidden_layers.1.bias, Mean Absolute Difference: 0.10686194896697998\n",
      "Parameter: hidden_layers.2.weight, Mean Absolute Difference: 0.13297316431999207\n",
      "Parameter: hidden_layers.2.bias, Mean Absolute Difference: 0.15499120950698853\n",
      "Parameter: hidden_layers.3.weight, Mean Absolute Difference: 0.14737039804458618\n",
      "Parameter: hidden_layers.3.bias, Mean Absolute Difference: 0.2247287631034851\n",
      "Parameter: hidden_layers.4.weight, Mean Absolute Difference: 0.1584581732749939\n",
      "Parameter: hidden_layers.4.bias, Mean Absolute Difference: 0.24859778583049774\n",
      "Parameter: hidden_layers.5.weight, Mean Absolute Difference: 0.25222891569137573\n",
      "Parameter: hidden_layers.5.bias, Mean Absolute Difference: 0.3081706762313843\n",
      "Parameter: hidden_layers.6.weight, Mean Absolute Difference: 0.47572603821754456\n",
      "Parameter: hidden_layers.6.bias, Mean Absolute Difference: 0.383177250623703\n",
      "Parameter: output_layer.weight, Mean Absolute Difference: 0.6171779036521912\n",
      "Parameter: output_layer.bias, Mean Absolute Difference: 1.1854283809661865\n",
      "\n",
      "local -- score --\n",
      "Parameter: input_layer.weight, Mean Absolute Difference: 0.07729852199554443\n",
      "Parameter: input_layer.bias, Mean Absolute Difference: 0.16194993257522583\n",
      "Parameter: hidden_layers.0.weight, Mean Absolute Difference: 0.07375116646289825\n",
      "Parameter: hidden_layers.0.bias, Mean Absolute Difference: 0.09053769707679749\n",
      "Parameter: hidden_layers.1.weight, Mean Absolute Difference: 0.07509127259254456\n",
      "Parameter: hidden_layers.1.bias, Mean Absolute Difference: 0.07741089165210724\n",
      "Parameter: hidden_layers.2.weight, Mean Absolute Difference: 0.08309391140937805\n",
      "Parameter: hidden_layers.2.bias, Mean Absolute Difference: 0.06906585395336151\n",
      "Parameter: hidden_layers.3.weight, Mean Absolute Difference: 0.10140877962112427\n",
      "Parameter: hidden_layers.3.bias, Mean Absolute Difference: 0.08397278189659119\n",
      "Parameter: hidden_layers.4.weight, Mean Absolute Difference: 0.1309191733598709\n",
      "Parameter: hidden_layers.4.bias, Mean Absolute Difference: 0.08892926573753357\n",
      "Parameter: hidden_layers.5.weight, Mean Absolute Difference: 0.15375064313411713\n",
      "Parameter: hidden_layers.5.bias, Mean Absolute Difference: 0.09639231115579605\n",
      "Parameter: hidden_layers.6.weight, Mean Absolute Difference: 0.18622201681137085\n",
      "Parameter: hidden_layers.6.bias, Mean Absolute Difference: 0.14278267323970795\n",
      "Parameter: output_layer.weight, Mean Absolute Difference: 0.23925790190696716\n",
      "Parameter: output_layer.bias, Mean Absolute Difference: 0.24216383695602417\n",
      "\n",
      "target -- dice --\n",
      "Parameter: input_layer.weight, Mean Absolute Difference: 0.22622476518154144\n",
      "Parameter: input_layer.bias, Mean Absolute Difference: 0.23150108754634857\n",
      "Parameter: hidden_layers.0.weight, Mean Absolute Difference: 0.12451563775539398\n",
      "Parameter: hidden_layers.0.bias, Mean Absolute Difference: 0.11358191072940826\n",
      "Parameter: hidden_layers.1.weight, Mean Absolute Difference: 0.12343466281890869\n",
      "Parameter: hidden_layers.1.bias, Mean Absolute Difference: 0.120306596159935\n",
      "Parameter: hidden_layers.2.weight, Mean Absolute Difference: 0.1432587206363678\n",
      "Parameter: hidden_layers.2.bias, Mean Absolute Difference: 0.16450896859169006\n",
      "Parameter: hidden_layers.3.weight, Mean Absolute Difference: 0.16790321469306946\n",
      "Parameter: hidden_layers.3.bias, Mean Absolute Difference: 0.22791787981987\n",
      "Parameter: hidden_layers.4.weight, Mean Absolute Difference: 0.17704230546951294\n",
      "Parameter: hidden_layers.4.bias, Mean Absolute Difference: 0.25619542598724365\n",
      "Parameter: hidden_layers.5.weight, Mean Absolute Difference: 0.2670130729675293\n",
      "Parameter: hidden_layers.5.bias, Mean Absolute Difference: 0.3092089295387268\n",
      "Parameter: hidden_layers.6.weight, Mean Absolute Difference: 0.48952993750572205\n",
      "Parameter: hidden_layers.6.bias, Mean Absolute Difference: 0.3902943432331085\n",
      "Parameter: output_layer.weight, Mean Absolute Difference: 0.6250485777854919\n",
      "Parameter: output_layer.bias, Mean Absolute Difference: 1.1617182493209839\n",
      "\n",
      "target -- score --\n",
      "Parameter: input_layer.weight, Mean Absolute Difference: 0.18549415469169617\n",
      "Parameter: input_layer.bias, Mean Absolute Difference: 0.23481422662734985\n",
      "Parameter: hidden_layers.0.weight, Mean Absolute Difference: 0.08745431900024414\n",
      "Parameter: hidden_layers.0.bias, Mean Absolute Difference: 0.10331778973340988\n",
      "Parameter: hidden_layers.1.weight, Mean Absolute Difference: 0.08692783117294312\n",
      "Parameter: hidden_layers.1.bias, Mean Absolute Difference: 0.09411889314651489\n",
      "Parameter: hidden_layers.2.weight, Mean Absolute Difference: 0.09529075026512146\n",
      "Parameter: hidden_layers.2.bias, Mean Absolute Difference: 0.08614741265773773\n",
      "Parameter: hidden_layers.3.weight, Mean Absolute Difference: 0.12227815389633179\n",
      "Parameter: hidden_layers.3.bias, Mean Absolute Difference: 0.11278219521045685\n",
      "Parameter: hidden_layers.4.weight, Mean Absolute Difference: 0.14725558459758759\n",
      "Parameter: hidden_layers.4.bias, Mean Absolute Difference: 0.10896509885787964\n",
      "Parameter: hidden_layers.5.weight, Mean Absolute Difference: 0.17022550106048584\n",
      "Parameter: hidden_layers.5.bias, Mean Absolute Difference: 0.11474050581455231\n",
      "Parameter: hidden_layers.6.weight, Mean Absolute Difference: 0.2113545536994934\n",
      "Parameter: hidden_layers.6.bias, Mean Absolute Difference: 0.17186224460601807\n",
      "Parameter: output_layer.weight, Mean Absolute Difference: 0.2607153356075287\n",
      "Parameter: output_layer.bias, Mean Absolute Difference: 0.2489931881427765\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for q_type, action_type in itertools.product(['local', 'target'], ['dice', 'score']):\n",
    "\n",
    "    if q_type == 'local':\n",
    "        if action_type == 'dice':\n",
    "            initial = init_local_params_dice\n",
    "            current = current_local_params_dice\n",
    "        elif action_type == 'score':\n",
    "            initial = init_local_params_score\n",
    "            current = current_local_params_score\n",
    "    elif q_type == 'target':\n",
    "        if action_type == 'dice':\n",
    "            initial = init_target_params_dice\n",
    "            current = current_target_params_dice\n",
    "        elif action_type == 'score':\n",
    "            initial = init_target_params_score\n",
    "            current = current_target_params_score\n",
    "\n",
    "    print(f'\\n{q_type} -- {action_type} --')\n",
    "    param_differences = {}\n",
    "    for param_name in initial:\n",
    "        initial_param = initial[param_name]\n",
    "        current_param = current[param_name]\n",
    "        param_differences[param_name] = (current_param - initial_param).abs().mean().item()\n",
    "\n",
    "    # Print the parameter differences\n",
    "    for param_name, difference in param_differences.items():\n",
    "        print(f\"Parameter: {param_name}, Mean Absolute Difference: {difference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0.000000, Expected: 4.563137\n",
      "Target: 0.000000, Expected: 8.807869\n",
      "Target: 0.000000, Expected: 3.096425\n",
      "Target: 20.993841, Expected: 23.677082\n",
      "Target: 23.037207, Expected: 44.352177\n",
      "Target: 7323.008301, Expected: 7321.584473\n",
      "Target: 15.485868, Expected: 16.178547\n",
      "Target: 9.894644, Expected: 13.301423\n",
      "Target: 31.957365, Expected: 35.760323\n",
      "Target: 27.260185, Expected: 23.167732\n",
      "Target: 257.889679, Expected: 218.414352\n",
      "Target: 45.344055, Expected: 40.298233\n",
      "Target: 12.252589, Expected: 11.616371\n",
      "Target: 9.752342, Expected: 13.401367\n",
      "Target: 13.275702, Expected: 18.384567\n",
      "Target: 7.278747, Expected: 6.752117\n",
      "Target: 12.592152, Expected: 12.746131\n",
      "Target: 13.381138, Expected: 11.649648\n",
      "Target: 21.992617, Expected: 43.284378\n",
      "Target: 21.509319, Expected: 24.095362\n",
      "Target: 344.221466, Expected: 261.928223\n",
      "Target: 0.477121, Expected: -4.799522\n",
      "Target: 16.844887, Expected: 20.853764\n",
      "Target: 17.271811, Expected: 20.563585\n",
      "Target: 127.087173, Expected: 68.068359\n",
      "Target: 21.145144, Expected: 20.879972\n",
      "Target: 12.167798, Expected: 8.931952\n",
      "Target: 1.000000, Expected: 6.402344\n",
      "Target: 174.614426, Expected: 151.665833\n",
      "Target: 20.572872, Expected: 29.133781\n",
      "Target: 28.061119, Expected: 28.417971\n",
      "Target: 11.790162, Expected: 17.525103\n",
      "Target: 0.000000, Expected: 9.158600\n",
      "Target: 81.833649, Expected: 86.882721\n",
      "Target: 33.938053, Expected: 36.275085\n",
      "Target: 14.474818, Expected: 15.219120\n",
      "Target: 20.637648, Expected: 28.642406\n",
      "Target: 16.285540, Expected: 13.093863\n",
      "Target: 0.000000, Expected: 5.340759\n",
      "Target: 23.163534, Expected: 35.260300\n",
      "Target: 705.355530, Expected: 682.214233\n",
      "Target: 44.627079, Expected: 42.885342\n",
      "Target: 15.178484, Expected: 18.971935\n",
      "Target: 22.733351, Expected: 37.870613\n",
      "Target: 13.265151, Expected: 10.814082\n",
      "Target: 171.894028, Expected: 161.069427\n",
      "Target: 47.458035, Expected: 43.607300\n",
      "Target: 14.460265, Expected: 13.950892\n",
      "Target: 71.758858, Expected: 90.236916\n",
      "Target: 19.807446, Expected: 21.371078\n",
      "Target: 102.836884, Expected: 111.219604\n",
      "Target: 39.112373, Expected: 31.846432\n",
      "Target: 2077.989014, Expected: 2051.162354\n",
      "Target: 14.158115, Expected: 27.516167\n",
      "Target: 0.000000, Expected: 6.116019\n",
      "Target: 18.578594, Expected: 11.581897\n",
      "Target: 30.563101, Expected: 49.181438\n",
      "Target: 11.559601, Expected: 13.262962\n",
      "Target: 13.820863, Expected: 21.625324\n",
      "Target: 60.942616, Expected: 115.647972\n",
      "Target: 16.965796, Expected: 17.696552\n",
      "Target: 11.421529, Expected: 16.286510\n",
      "Target: 12.752038, Expected: 10.637783\n",
      "Target: 1096.484009, Expected: 1026.600830\n",
      "Target: 17.511147, Expected: 12.863209\n",
      "Target: 42.074493, Expected: 36.971851\n",
      "Target: 90.949577, Expected: 136.120071\n",
      "Target: 17.622019, Expected: 32.664661\n",
      "Target: 27.675657, Expected: 37.781563\n",
      "Target: 1290.154663, Expected: 1274.367065\n",
      "Target: 21.621435, Expected: 24.594303\n",
      "Target: 53.103447, Expected: 65.960129\n",
      "Target: 0.698970, Expected: 10.414005\n",
      "Target: 347.135254, Expected: 308.905365\n",
      "Target: 12.658719, Expected: 14.058388\n",
      "Target: 13.565625, Expected: 17.586123\n",
      "Target: 19.041851, Expected: 13.366614\n",
      "Target: 12.779746, Expected: 8.422108\n",
      "Target: 0.000000, Expected: 8.989922\n",
      "Target: 0.000000, Expected: 22.620447\n",
      "Target: 15.204835, Expected: 15.411632\n",
      "Target: 29.537636, Expected: 29.094904\n",
      "Target: 14.693933, Expected: 11.804766\n",
      "Target: 15.510110, Expected: 9.622753\n",
      "Target: 20.591339, Expected: 15.821876\n",
      "Target: 0.000000, Expected: 18.762457\n",
      "Target: 76.723167, Expected: 60.506741\n",
      "Target: 12.235648, Expected: 12.569145\n",
      "Target: 80.710320, Expected: 85.980461\n",
      "Target: 2691.694580, Expected: 2790.261230\n",
      "Target: 20.019207, Expected: 36.907280\n",
      "Target: 16.188019, Expected: 17.572199\n",
      "Target: 8315.051758, Expected: 8234.901367\n",
      "Target: 10.531854, Expected: 8.218462\n",
      "Target: 82.710800, Expected: 74.355385\n",
      "Target: 2729.470703, Expected: 2673.678223\n",
      "Target: 23.831009, Expected: 16.872852\n",
      "Target: 16.425522, Expected: 16.090708\n",
      "Target: 35.098831, Expected: 33.903774\n",
      "Target: 1913.089233, Expected: 1842.125977\n",
      "Target: 12.298615, Expected: 9.197411\n",
      "Target: 44.738686, Expected: 56.717201\n",
      "Target: 10.480556, Expected: 9.599451\n",
      "Target: 24.104771, Expected: 43.206245\n",
      "Target: 10.534964, Expected: 2.202760\n",
      "Target: 1990.573730, Expected: 1961.065308\n",
      "Target: 12.509220, Expected: 13.547073\n",
      "Target: 188.928391, Expected: 105.096870\n",
      "Target: 126.836899, Expected: 21.410004\n",
      "Target: 13.565604, Expected: 11.893013\n",
      "Target: 39.115452, Expected: 68.147537\n",
      "Target: 499.478668, Expected: 411.442322\n",
      "Target: 13.931377, Expected: 13.838242\n",
      "Target: 25.800512, Expected: -29.420843\n",
      "Target: 10.329308, Expected: 5.971561\n",
      "Target: 717.469666, Expected: 664.843140\n",
      "Target: 14.369717, Expected: 28.783808\n",
      "Target: 2015.919678, Expected: 2010.074097\n",
      "Target: 19.023243, Expected: 17.750254\n",
      "Target: 10.784848, Expected: -4.294296\n",
      "Target: 99.144745, Expected: 95.893814\n",
      "Target: 921.990723, Expected: 936.726501\n",
      "Target: 102.687859, Expected: 83.982468\n",
      "Target: 75.259354, Expected: 180.204636\n",
      "Target: 11.585319, Expected: 6.005016\n",
      "Target: 9.449065, Expected: 6.852035\n",
      "Target: 128.910034, Expected: 71.555252\n",
      "Target: 267.465027, Expected: 331.660339\n",
      "Target: 11.998095, Expected: 8.757101\n",
      "Target: 17.899269, Expected: 20.584047\n",
      "Target: 16.170221, Expected: 65.740784\n",
      "Target: 716.419128, Expected: 629.765686\n",
      "Target: 11.818295, Expected: 13.024185\n",
      "Target: 1485.172729, Expected: 1416.558838\n",
      "Target: 1663.787354, Expected: 1700.539185\n",
      "Target: 12.318705, Expected: 7.533974\n",
      "Target: 66.780655, Expected: 83.121521\n",
      "Target: 0.000000, Expected: 6.703986\n",
      "Target: 13.136077, Expected: 9.207705\n",
      "Target: 14.547812, Expected: 12.551417\n",
      "Target: 15.361499, Expected: 11.008115\n",
      "Target: 853.230713, Expected: 823.355408\n",
      "Target: 30.289461, Expected: 22.439705\n",
      "Target: 14.574808, Expected: 20.271801\n",
      "Target: 136.926956, Expected: 73.523148\n",
      "Target: 106.304298, Expected: 34.690094\n",
      "Target: 126.276100, Expected: 322.167084\n",
      "Target: 43.796181, Expected: 38.056190\n",
      "Target: 230.935242, Expected: 164.987015\n",
      "Target: 13.173945, Expected: 6.745485\n",
      "Target: 127.173340, Expected: 106.876846\n",
      "Target: 26.046331, Expected: 13.298227\n",
      "Target: 16.795250, Expected: 26.065254\n",
      "Target: 60.331619, Expected: 53.881771\n",
      "Target: 12.327331, Expected: 12.569954\n",
      "Target: 2743.371094, Expected: 2737.444336\n",
      "Target: 66.007416, Expected: 50.318367\n",
      "Target: 67.680840, Expected: 48.145065\n",
      "Target: 91.465286, Expected: 30.007618\n",
      "Target: 33.435242, Expected: 43.820328\n",
      "Target: 13.345032, Expected: 12.864165\n",
      "Target: 12.938116, Expected: 13.403251\n",
      "Target: 0.000000, Expected: 45.896412\n",
      "Target: 17.028210, Expected: 11.254539\n",
      "Target: 1.041393, Expected: 13.424504\n",
      "Target: 10.481750, Expected: 20.488152\n",
      "Target: 28.622910, Expected: 28.430668\n",
      "Target: 19.405430, Expected: 24.706284\n",
      "Target: 26.914906, Expected: 24.136219\n",
      "Target: 0.477121, Expected: 25.133949\n",
      "Target: 12.411366, Expected: 14.670742\n",
      "Target: 7.645007, Expected: 10.159646\n",
      "Target: 17.547667, Expected: 46.035088\n",
      "Target: 11.850390, Expected: 13.443229\n",
      "Target: 15.561988, Expected: 9.580814\n",
      "Target: 12.548604, Expected: 10.011017\n",
      "Target: 767.569824, Expected: 681.356934\n",
      "Target: 1398.395752, Expected: 1322.897949\n",
      "Target: 11.972122, Expected: 7.528614\n",
      "Target: 13.690986, Expected: 12.674842\n",
      "Target: 10.992841, Expected: 14.915519\n",
      "Target: 24.733923, Expected: 16.958075\n",
      "Target: 112.645111, Expected: 113.894241\n",
      "Target: 29.501776, Expected: 23.446604\n",
      "Target: 24.183315, Expected: 31.023855\n",
      "Target: 75.701996, Expected: 47.995220\n",
      "Target: 39.302113, Expected: 46.082623\n",
      "Target: 16.166803, Expected: 10.736170\n",
      "Target: 10.925706, Expected: 8.797557\n",
      "Target: 38.083179, Expected: 52.218655\n",
      "Target: 22.969208, Expected: 18.577999\n",
      "Target: 22.556810, Expected: 28.301970\n",
      "Target: 0.000000, Expected: 9.142921\n",
      "Target: 128.069199, Expected: 155.371536\n",
      "Target: 0.000000, Expected: 1.093287\n",
      "Target: 43.756981, Expected: 41.864445\n",
      "Target: 10.744992, Expected: 11.217606\n",
      "Target: 18.295067, Expected: 70.750771\n",
      "Target: 11.927203, Expected: 10.509521\n",
      "Target: 33.511780, Expected: 29.767447\n",
      "Target: 14.400393, Expected: 16.432570\n",
      "Target: 19.503952, Expected: 17.905544\n",
      "Target: 13.962462, Expected: 19.562744\n",
      "Target: 704.763428, Expected: 675.601929\n",
      "Target: 6.511491, Expected: 9.656802\n",
      "Target: 37.130585, Expected: 28.302567\n",
      "Target: 14.572214, Expected: 18.367424\n",
      "Target: 0.000000, Expected: 7.174706\n",
      "Target: 3145.618408, Expected: 3092.969727\n",
      "Target: 17.319208, Expected: 12.289680\n",
      "Target: 16.652122, Expected: 7.742454\n",
      "Target: 9.686230, Expected: 8.672953\n",
      "Target: 60.183197, Expected: 68.933113\n",
      "Target: 19.392532, Expected: 11.787832\n",
      "Target: 24.217693, Expected: 21.058922\n",
      "Target: 540.887024, Expected: 635.287842\n",
      "Target: 15.110813, Expected: 15.236538\n",
      "Target: 15.037512, Expected: 17.147156\n",
      "Target: 22.682592, Expected: 15.070297\n",
      "Target: 1152.820557, Expected: 1227.339478\n",
      "Target: 0.477121, Expected: 6.140046\n",
      "Target: 302.681946, Expected: 278.599670\n",
      "Target: 30.771532, Expected: 37.216476\n",
      "Target: 3956.412109, Expected: 3974.779297\n",
      "Target: 14.326874, Expected: 7.890919\n",
      "Target: 22.581505, Expected: 14.917618\n",
      "Target: 120.491158, Expected: 171.684647\n",
      "Target: 15.583125, Expected: 13.426333\n",
      "Target: 49.131653, Expected: 92.034424\n",
      "Target: 6.735735, Expected: 8.018932\n",
      "Target: 16.361938, Expected: 18.671301\n",
      "Target: 15.325959, Expected: 11.309882\n",
      "Target: 1443.428345, Expected: 1487.169434\n",
      "Target: 10.559966, Expected: 7.749197\n",
      "Target: 12.296499, Expected: 15.460400\n",
      "Target: 24.059999, Expected: 20.435923\n",
      "Target: 1648.506592, Expected: 1635.787720\n",
      "Target: 17.235424, Expected: 15.480793\n",
      "Target: 39.418148, Expected: 13.475702\n",
      "Target: 12.739574, Expected: 8.159433\n",
      "Target: 32.661018, Expected: 51.012482\n",
      "Target: 1929.807007, Expected: 1937.215210\n",
      "Target: 22.083652, Expected: -0.551306\n",
      "Target: 8.850470, Expected: 7.091110\n",
      "Target: 15.603035, Expected: 15.267170\n",
      "Target: 14.933023, Expected: 11.569298\n",
      "Target: 0.000000, Expected: 5.490299\n",
      "Target: 54.314445, Expected: 77.357857\n",
      "Target: 17.919821, Expected: 12.602543\n",
      "Target: 13.109652, Expected: 9.945420\n",
      "Target: 17.035545, Expected: 21.523203\n",
      "Target: 2512.357178, Expected: 2316.013184\n",
      "Target: 11.521358, Expected: 14.218828\n",
      "Target: 10.184232, Expected: 1.713580\n",
      "Target: 13.734674, Expected: 9.466380\n",
      "Target: 12.096213, Expected: 7.570337\n"
     ]
    }
   ],
   "source": [
    "for target, expected in zip(Q_targets, Q_expected):\n",
    "    print(f'Target: {target.item():4f}, Expected: {expected.item():4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1064.2625, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = Q_expected - Q_targets\n",
    "errors_squared = errors**2\n",
    "errors_squared.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1064.2625, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = agent.qnetwork_local_dice\n",
    "target_model = agent.qnetwork_target_dice\n",
    "tau = TAU\n",
    "\n",
    "for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "    target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data) # or + tau?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "for experience in agent.memory_dice.memory:\n",
    "    state, action, reward, next_state, done = experience\n",
    "    result = pd.DataFrame({'state': state, 'action': action, 'reward': reward, 'next_state': next_state, 'done': done})\n",
    "    results = pd.concat([results, result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),\n",
       " array([ 4380,  5860,  4580,  3740,  5160,  3720,  4740,  3720,  3960,\n",
       "        12640,  4340,  3420, 10680, 11260,  4000,  8160,  3340,  4580,\n",
       "         3480,  4660,  3520,  4180,  6940,  3740,  4540,  3660,  5700,\n",
       "         3920, 22180,  9160,  4920, 17120]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(results.action, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.bar(np.unique(results.action, return_counts=True)[0], np.unique(results.action, return_counts=True)[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(results.reward)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botzee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
